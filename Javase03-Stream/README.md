# 📅 2026.01.02: Stream 流与 AI 数据清洗

## 🎯 今日目标
- [x] 理解 Stream 的 **流水线 (Pipeline)** 模型
- [x] 掌握常用中间操作（`filter`, `map`, `distinct`, `sorted`）
- [x] 实战：实现 **AI 生成数据的自动化清洗过滤**

## 💡 核心洞察 (Insight)
* **从“如何做”到“做什么”**：Stream 让我们告别了臃肿的 `for` 循环和 `if` 嵌套，代码具备了极强的可读性和工程美感。
* **数据脱敏与预处理**：在 AI 场景下，Stream 是将非结构化文本转化为结构化特征（Feature）的第一道工序。

## 🎙️ 面试高频考点
1. **中间操作与终止操作的区别**：中间操作返回 Stream 且延迟执行；终止操作触发计算并返回结果。
2. **Stream 的性能**：对于小数据量，Stream 可能比传统 for 循环略慢（由于对象创建），但其**代码维护性**和**易于并行化**的优势在大型 AI 工程中无可替代。
3. **flatMap 原理** : 1对N转换，将嵌套集合“摊平”为单一流。 
4. **短路操作** : `anyMatch/findFirst` 等操作在满足条件时立即返回，无需遍历全量数据。 
5. **并行流风险** : 共享公共线程池，滥用会导致 IO 密集型任务阻塞整个应用；需注意线程安全。 
6. **分组统计** : `groupingBy` 结合 `Collectors` 可实现类似 SQL 的 `GROUP BY` 功能。

## 🧠 防遗忘附录（Stream · 工程 ETL 向）

### 我当时在干什么？
- 用 Stream 清洗 AI 生成的原始数据
- 处理脏数据、空值、重复值
- 把“杂乱输入”变成“可用结果”

---

### 一开始我哪里想错了？
- 把 Stream 当成 for 循环的语法糖
- 没意识到 Stream 是 **数据流水线**
- 没想清楚每一步转换的职责

---

### 我最后记住的 3 句话
- Stream 是“声明式数据处理管道”
- 每一步只做一件事
- `distinct()` 底层依赖的还是 Set 思想

---

### 工程 / AI 场景中的 Stream
- AI 输出数据清洗（ETL）
- 日志分析与过滤
- 批量数据预处理（RAG 前置步骤）

---

### 关键代码锚点
- AI 数据清洗示例：`AiDataStreamer.java`

---

### 如果以后全忘了
👉 顺着 pipeline 问自己：  
**“这一行代码，负责把数据‘变成什么’？”**
